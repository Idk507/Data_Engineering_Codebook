

---

## ✅ Step-by-Step Guide to Install PySpark and Run First Program

---

### **1. Pre-requisites**

Ensure the following are installed:

* **Python 3.7+**
* **Java 8 or 11 (JDK)** (Spark runs on JVM)
* **pip** (Python package manager)

---

### **2. Install Java**

#### **Linux/Mac:**

```bash
sudo apt update
sudo apt install openjdk-11-jdk -y
```

Verify installation:

```bash
java -version
```

#### **Windows:**

1. Download and install Java 8 or 11 JDK from: [https://adoptopenjdk.net](https://adoptopenjdk.net) or [https://www.oracle.com/java/technologies/javase-jdk11-downloads.html](https://www.oracle.com/java/technologies/javase-jdk11-downloads.html)
2. Set `JAVA_HOME` environment variable:

   * Search "Environment Variables"
   * Add new system variable:

     * `JAVA_HOME`: `C:\Program Files\Java\jdk-11.0.x`
     * Add `JAVA_HOME\bin` to your `Path` variable

---

### **3. Install PySpark via pip**

```bash
pip install pyspark
```

This installs:

* PySpark
* All dependencies including Py4J (for JVM communication)

---

### **4. Optional (For Jupyter Support)**

Install Jupyter and IPython kernel if using notebooks:

```bash
pip install notebook ipykernel
python -m ipykernel install --user
```

---

### **5. Verify Installation (Command Line)**

#### Start Python REPL:

```bash
python
```

#### Run:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MyFirstApp").getOrCreate()
print(spark.version)
spark.stop()
```

If this runs without error and prints the Spark version — installation is successful.

---

### **6. Run First PySpark Program (Script or Notebook)**

#### **First PySpark Script:**

Create a file called `first_app.py`:

```python
from pyspark.sql import SparkSession

# Step 1: Start a Spark session
spark = SparkSession.builder \
    .appName("First PySpark App") \
    .getOrCreate()

# Step 2: Create a simple DataFrame
data = [("Alice", 25), ("Bob", 30), ("Charlie", 28)]
columns = ["Name", "Age"]

df = spark.createDataFrame(data, columns)

# Step 3: Show the DataFrame
df.show()

# Step 4: Stop the Spark session
spark.stop()
```

#### Run it from terminal:

```bash
python first_app.py
```

#### Output:

```
+-------+---+
|   Name|Age|
+-------+---+
|  Alice| 25|
|    Bob| 30|
|Charlie| 28|
+-------+---+
```

---

### **7. Run in Jupyter Notebook (Optional)**

#### Open Jupyter:

```bash
jupyter notebook
```

Create a new notebook and run:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("NotebookApp").getOrCreate()

data = [("India", 1.4), ("USA", 0.33), ("China", 1.42)]
columns = ["Country", "Population (B)"]

df = spark.createDataFrame(data, columns)
df.show()

spark.stop()
```

---

### **8. Tips for Smooth Setup**

* If using **VS Code**, install the **Python** and **Jupyter** extensions
* For **Windows** users: if facing path issues, try using **WSL** (Windows Subsystem for Linux) or run in Anaconda
* You can set Spark options with `.config()` inside `SparkSession.builder`

---

## ✅ Summary

| Step | Description                                               |
| ---- | --------------------------------------------------------- |
| 1    | Install Java JDK 8/11                                     |
| 2    | Install PySpark via `pip install pyspark`                 |
| 3    | Test using Python shell or Jupyter                        |
| 4    | Run your first PySpark program                            |
| 5    | Use `SparkSession` to create DataFrame and call `.show()` |

---

